This thesis presents a modular, multi-agent system for converting unstructured spoken language into structured templates using large language models (LLMs). The focus is on real-world domains such as healthcare, manufacturing, and administration. Traditional monolithic LLM approaches often struggle with ambiguous or noisy inputs, lack adaptability, and provide limited transparency. To address these limitations, the proposed Invox system decomposes the task into four subtasks—transcription interpretation, field mapping, value inference, and result verification—handled by specialized LLM agents. Five architectural strategies are implemented and compared: Single-Pass Full Input, Iterative Single-Field Processing, Multi-LLM Consensus (Full), Multi-LLM Consensus (Iterative), and Hybrid Refinement. The system integrates tools such as Whisper, GPT-4, Claude, and DeepSeek, and is evaluated across benchmarks including the MUC-4 dataset and a real-world industrial corpus from steel manufacturing. The evaluation focuses on accuracy, consistency, latency, cost-efficiency, and modularity, offering insight into the trade-offs and design choices involved in deploying agent-based LLM architectures for speech-driven template filling.

\textbf{Keywords: Large Language Models (LLMs), Multi-Agent Systems, Template Filling, Prompt Engineering, Natural Language Understanding, Speech-to-Text Processing}