Information extraction refers to the framework's ability to accurately identify and populate relevant template fields from unstructured or semi-structured input, while leaving irrelevant fields empty. This process goes beyond simple keyword matching and instead relies on linguistic context, entity relationships, and domain-specific semantics to ensure that extracted values are both accurate and meaningful~\cite{grishman1997information, sarawagi2008information}. In practice, information extraction determines whether heterogeneous and noisy inputs—such as partially transcribed voice recordings, fragmented text segments, or incomplete chat messages—can be transformed into reliable structured data~\cite{jurafsky2023speech, liu2022conversational}.

As illustrated in Figure~\ref{fig:form-filling-example}, an ideal solution must be able to extract relevant details even when input is noisy, fragmented, or incomplete, and populate the corresponding template fields while leaving others empty. The highlighted gaps in the example show that extraction is not only about finding correct values but also about reliably detecting when information is missing and avoiding unsupported guesses.

This requirement matters because real-world text is rarely clean or complete. Voice transcriptions may drop words or introduce errors, workplace chat logs often contain fragments, abbreviations, or extraneous commentary~\cite{baldwin2013noisy}. Without strong extraction capabilities, systems risk either leaving critical fields blank or, worse, filling them with incorrect assumptions. In regulated or safety-critical domains such as healthcare or industrial maintenance, such errors can result in compliance violations, misdiagnoses, or operational risks~\cite{radford2023whisper, fathullah2023prompting}.

In practice, effective extraction can be achieved through three complementary mechanisms. First, the system must be robust to incomplete input, recognizing when essential details are missing and preserving empty fields instead of filling them with fabricated values. For example, if a date of inspection is not provided, the system should explicitly leave the field empty rather than guessing. Second, conflict handling is essential. If a transcript contains contradictory information—such as "appointment on August 10" followed by "meeting scheduled for August 12"—the system should not arbitrarily choose one date but instead detect the inconsistency and prompt the user for clarification, ensuring reliability of the final record. Third, extraction requires contextual understanding. Information is often scattered across multiple parts of a conversation or document, and the system must combine relevant fragments into coherent field values while ignoring unrelated chatter. For instance, if a technician states, "We restarted the press at 23:14. Oh, and the noise got louder after ten minutes," the system must connect these details to the same inspection event rather than treating them as separate or unrelated entries.

The advantages of strong information extraction capabilities extend across multiple dimensions. They improve accuracy by minimizing both incorrect and missing field values, even when input is fragmented or linguistically complex. They reduce user workload by lowering the amount of manual correction required after automatic field population. They enhance robustness to noise by maintaining performance in the presence of typos, filler words, or irrelevant side remarks. Finally, they enable better multi-source integration by combining relevant information from different segments into a single coherent template entry, which is particularly valuable in collaborative or multi-turn interactions~\cite{shneiderman2016designing}.

For evaluation, we measure semantic correctness through cosine similarity of embeddings computed at the field level. For each template field, both the complete system output and the complete gold standard answer are converted into dense vector representations using pre-trained sentence embedding models (e.g., Sentence-BERT, Universal Sentence Encoder). This approach appropriately credits semantically equivalent extractions such as "machine overheating" versus "equipment temperature exceeds threshold," as it is robust to paraphrasing and stylistic variation. Other similar benchmarks measuring information correctness and semantic alignment can also be used depending on the domain and available resources.

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.6}
\setlength{\tabcolsep}{12pt}
\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}m{3cm}|>{\arraybackslash}X|}
\hline
\textbf{Visual Score} & \textbf{Interpretation (based on criteria)} \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} 
& \textbf{All three criteria satisfied.} The system reliably handles incomplete text, correctly detects and flags conflicts, and successfully integrates related information spread across different text parts. Semantic similarity scores are high (e.g., cosine similarity $\geq 0.75$). \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) -- (90:0.4cm) arc (90:-150:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}} 
& \textbf{Two criteria satisfied.} For instance, the system handles incomplete input and detects conflicts but fails to integrate scattered contextual information. Semantic similarity scores are moderate (e.g., cosine similarity in $[0.60, 0.75)$). \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) -- (90:0.4cm) arc (90:-30:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}} 
& \textbf{One criterion satisfied.} For instance, the system handles incomplete text but fails to detect conflicts and integrate contextual information. Semantic similarity scores are low (e.g., cosine similarity in $[0.45, 0.60)$). \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} 
& \textbf{No criteria satisfied.} The system fails to handle incomplete input, cannot detect conflicts, and does not integrate information spread across different parts of the input. Semantic similarity scores are very low (e.g., cosine similarity $< 0.45$). \\
\hline
\end{tabularx}
\caption{Evaluation scale for R2: Information Extraction Abilities.}
\label{tab:r2-extraction-metrics}
\end{table}

To summarize, information extraction abilities determine whether heterogeneous and noisy user inputs can be reliably transformed into structured templates. A robust system not only identifies and assigns relevant details but also recognizes gaps, resolves contradictions, and integrates scattered fragments into coherent entries. By measuring semantic correctness through embedding-based similarity metrics at the field level, we can evaluate whether extracted values capture the intended meaning regardless of stylistic variation. This reduces user workload, improves reliability, and ensures that the extracted data supports both operational decision-making and automated analysis.