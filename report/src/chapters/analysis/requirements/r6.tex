\subsection*{R6: Usability}

Usability in the context of this framework refers primarily to \textbf{latency}, i.e., the time delay between user input and the system’s output (e.g., updated template fields becoming visible). While this thesis evaluates a speech-driven scenario, \emph{speech is only one use case}; the core task is the transformation of \emph{unstructured input (spoken or typed text)} into structured templates. Accordingly, the latency requirement applies to any unstructured-to-template pipeline (typed submissions, transcripts, or mixed modalities). In such settings, responsiveness directly determines whether interaction feels natural and uninterrupted. Human–computer interaction research consistently shows that feedback within two to three seconds is perceived as natural turn-taking, while longer delays disrupt concentration and diminish trust in the system~\cite{shneiderman2016designing}. For voice-first applications, user expectations are shaped by everyday experiences with assistants, where near-instantaneous responses are the norm~\cite{wang2021slu}.

As illustrated in Figure~\ref{fig:form-filling-example}, the template should update promptly after a spoken or typed submission. Timely visual feedback preserves flow and enables quick verification.

Unlike accuracy or error tolerance, which are covered in other requirements (R1–R5), usability here is scoped strictly to latency. This requirement matters because template filling often occurs in time-pressured contexts (e.g., hospitals or industrial facilities), where long waiting times interrupt workflows. For example, a technician dictating a machine fault—or a clerk pasting a long free-text note—must continue seamlessly; if the system takes ten or more seconds to update, the reporting process is disrupted and efficiency drops. Thus, latency is the decisive factor for adoption in real-world deployments.

However, latency is not an absolute property of the software alone. It is strongly influenced by the \textbf{hardware and deployment environment} on which inference is performed. High-performance accelerators such as GPUs and TPUs can significantly reduce end-to-end response time, whereas CPU-only or resource-constrained devices typically increase delays~\cite{shneiderman2016designing, jurafsky2023slp}. Therefore, when evaluating usability, latency benchmarks must always be reported together with the hardware profile (e.g., GPU/TPU type, number of concurrent users, and network setting). This ensures that usability scores are interpreted relative to realistic deployment contexts rather than as abstract performance claims.

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.6}
\setlength{\tabcolsep}{12pt}
\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}m{3cm}|>{\arraybackslash}X|}
\hline
\textbf{Visual Score} & \textbf{Interpretation} \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} 
& \textbf{High usability.} p95 end-to-end (E2E) latency $\leq$ 3\,s on the declared hardware profile. Interaction feels natural; conversational pacing is preserved. \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) -- (90:0.4cm) arc (90:-90:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}}
& \textbf{Medium usability.} p95 E2E latency in $(3,10)$\,s. Delay is noticeable but tolerable in most workflows. \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} 
& \textbf{No usability.} p95 E2E latency $>$ 10\,s or frequent timeouts. Workflow is impractical under this configuration. \\
\hline
\end{tabularx}
\caption{Evaluation Scale for R6: Usability}
\label{tab:r6-usability}
\end{table}

\paragraph{Evaluation method (latency definition).}
We measure \emph{end-to-end (E2E) latency} from the time the user finishes an utterance \emph{or submits text} until the corresponding template fields are rendered. For each system, p95 latency is reported (optionally p50 for typical-case summaries), ensuring responsiveness is captured consistently~\cite{amershi2019guidelines, jurafsky2023slp}. All results must specify the underlying hardware setup.

To summarize, usability in this framework is defined as responsiveness measured through latency for unstructured-to-template conversion across modalities (speech and text). Because latency is hardware-dependent, evaluation must always contextualize results by reporting the target infrastructure. This ensures comparability across systems while keeping interpretations grounded in practical deployment realities.
