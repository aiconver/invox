Consistency refers to the framework’s ability to transform heterogeneous and unstructured inputs—such as transcribed voice recordings, instant messages between employees, or informal written notes—into uniformly populated templates. Unlike free-text logs, templates require values that are standardized and comparable. This means that field entries must not vary arbitrarily in their wording, style, length, or level of specificity. This is a core aspect of what the research field calls "authorship attribution" in the context of LLMs, where the goal is to identify a consistent stylistic "fingerprint" from a given system~\cite{huang2024authorship}. Prior work in conversational systems has shown that a lack of normalization makes it difficult to align user inputs across contexts~\cite{liu2022conversational}, while studies in accessibility highlight how inconsistent representations reduce clarity and comparability for end users~\cite{clark2020accessible}. In the broader NLP field, research on natural language generation similarly stresses that variation without standardization complicates evaluation and downstream analysis~\cite{gatt2018survey}.


As illustrated in Figure~\ref{fig:form-filling-example}, an ideal solution should ensure that once information is extracted, the resulting field values are expressed in a consistent way. This means that answers are not only correct in content but also follow standardized wording, style, formatting, and level of detail. For example, dates should always appear in the same format, technical issues should be described with domain-specific terminology rather than colloquial phrasing, and descriptive fields should provide comparable levels of detail. Such uniform rendering makes entries easier to interpret for humans and more reliable to process automatically across different users, departments, or time periods.


This requirement is especially critical because heterogeneous sources of input inevitably produce inconsistent outputs if left unchecked. In industrial contexts, for example, one technician might describe a machine malfunction as a “slight issue,” another as a “minor fault,” and a third as a “mechanical malfunction.” Even if these phrases describe the same problem, their inconsistency complicates automated reporting and trend analysis, a challenge widely recognized in studies on data quality for industrial systems~\cite{norman2013design}. In healthcare, freeform reports such as “bad cough,” “patient has a cold,” or “respiratory illness” may all refer to the same condition. Medical informatics research has long emphasized that without concept normalization, such variations remain scattered as distinct entries and undermine clinical decision support~\cite{friedman2004survey}. More recent work on clinical text processing shows that mapping expressions to standardized terminologies is essential for interoperability and regulatory compliance~\cite{jonnalagadda2010medical, bodenreider2004unified}. Consistency therefore ensures that all such inputs are aligned to a common representational standard, enabling both human readability and machine-actionable analysis~\cite{shneiderman2016designing}.


In practice, consistency operates across multiple dimensions of representation. At the level of wording, normalization ensures that common phrases are mapped to standardized terminology. For example, “the patient has a cold” can be rendered as “patient with acute nasopharyngitis,” while “the thing is overheating” may be reformulated as “machine component exceeds temperature threshold.” At the level of tonality, consistency harmonizes register and style, replacing subjective or vague expressions such as “slight issue with machine” with standardized formulations such as “minor mechanical malfunction.” At the level of length, overly short entries like “headache” are enriched with sufficient detail to be meaningful, while excessively long descriptions such as “patient reports persistent mild headache for the past two days” are normalized into concise but precise formulations. Finally, at the semantic level, consistency promotes appropriate specificity, ensuring that generic categories like “illness” are consistently replaced with precise terms such as “influenza type B.”  

Formatting and style conventions are equally important to this requirement. Inconsistent formatting creates unnecessary barriers to readability and analysis. For instance, date entries should adhere to predefined standards so that “August 16th” is consistently represented as “08/16/2025.” Similarly, identifiers such as machine codes, patient IDs, or regulatory classification numbers should always follow the same syntactic pattern. Without such formatting alignment, even correctly normalized content may remain ambiguous or unusable across systems. Consistency therefore extends beyond lexical normalization to include structural and formal conventions that guarantee uniformity across the entire template.  

The advantages of consistent template filling are multifold. Consistency improves readability, as users can process and compare entries more quickly when values follow uniform patterns of wording and formatting. It reduces ambiguity by replacing vague or stylistically divergent expressions with precise, standardized equivalents, thereby lowering the risk of misunderstanding. It increases the usefulness of templates in professional contexts, ensuring that field entries are sufficiently detailed to meet regulatory, diagnostic, or legal requirements. Finally, it enhances comparability: homogeneous field values are easier to query, index, and aggregate, which facilitates automated reporting and large-scale data analysis across organizations and timeframes. 

\begin{table}[h!]
\centering
\renewcommand{\arraystretch}{1.6}
\setlength{\tabcolsep}{12pt}
\begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}m{3cm}|>{\arraybackslash}X|}
\hline
\textbf{Visual Score} & \textbf{Interpretation (with thresholds)} \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} 
& \textbf{High consistency.} Structured fields are correct in most cases (macro exact match/F1 $\geq 0.80$), and free-text fields show strong semantic and stylistic alignment (normalized BLEURT $\geq 0.70$ and Cosine Similarity $\geq 0.70$). \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) -- (90:0.4cm) arc (90:-150:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}} 
& \textbf{Medium consistency.} Structured fields show moderate accuracy (macro exact match/F1 in $[0.65, 0.80)$), and free-text fields achieve moderate semantic and stylistic similarity (normalized BLEURT in $[0.55, 0.70)$ and Cosine Similarity in $[0.55, 0.70)$). \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\filldraw[fill=black] (0,0) -- (90:0.4cm) arc (90:-30:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}} 
& \textbf{Low consistency.} Structured fields are often incorrect (macro exact match/F1 in $[0.50, 0.65)$), and free-text fields align poorly (normalized BLEURT in $[0.40, 0.55)$ and Cosine Similarity in $[0.40, 0.55)$). \\
\hline
\centering\raisebox{0pt}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} 
& \textbf{No consistency.} Structured accuracy is very low (macro exact match/F1 $<0.50$) and free-text similarity is weak (normalized BLEURT $<0.40$ and Cosine Similarity $<0.40$). Entries are unreliable for comparison or analysis. \\
\hline
\end{tabularx}
\caption{Evaluation scale for R1: Consistency.}
\label{tab:r1-consistency-thresholds}
\end{table}

For evaluation, we use exact match or F1 scores on structured fields such as identifiers, categories, and dates, and BLEURT on free-text fields such as descriptions or notes. Because raw BLEURT scores can vary depending on the checkpoint and dataset, we normalize them to $[0,1]$ across all system outputs in the test set.

To further evaluate the consistency of the system's output style—a key challenge in LLM-generated text—we also employ a stylometric analysis. Specifically, we measure the Cosine Similarity of the n-gram distributions between each system's output and the golden benchmark. Unlike Jaccard similarity, which only considers the presence or absence of n-grams, Cosine Similarity accounts for their frequency. N-grams, which are contiguous sequences of words, provide a quantitative measure of the system's syntactic style. A higher similarity score here indicates that the system's output is not only semantically correct (as measured by BLEURT) but also stylistically aligned with a human-authored standard, thus providing a more comprehensive evaluation of consistency.

To summarize, consistency ensures that heterogeneous, informal, or ambiguous user inputs are transformed into standardized, comparable, and semantically coherent template entries. By enforcing normalization across wording, tonality, length, semantic level, and formatting conventions, and by measuring agreement with the gold standard through exact match for structured fields and BLEURT for free-text fields, the system improves readability, minimizes ambiguity, and ensures that structured data can serve as a reliable foundation for both human decision-making and automated analysis.  
