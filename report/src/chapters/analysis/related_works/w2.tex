Large language models can generate structured outputs, but their reliability strongly depends on how the task is formulated. Prompt engineering has therefore become central for guiding LLMs toward schema-compliant and domain-aware outputs. By carefully designing instructions, examples, and schema descriptions, users can steer models to align their generations with the requirements of template filling.

The ability of LLMs to adapt to new tasks without retraining was first demonstrated through \emph{in-context learning}. Brown et al.\ showed that GPT-3 could perform unseen tasks when given only natural-language instructions and a few examples \cite{brown2020language}. Later, Wei et al.\ documented “emergent abilities,” where large models exhibit strong zero-shot and few-shot performance once surpassing certain scale thresholds \cite{wei2022emergent}. For information extraction, this means that models can often populate structured fields when the prompt clearly specifies the task and provides minimal demonstrations.

Designing effective prompts, however, is non-trivial. Sun et al.~\cite{sun2023slot} propose structured prompts for speech-based slot filling that incorporate task definitions, slot descriptions, and linearized knowledge, improving robustness to ASR noise. Similarly, Lin et al.~\cite{lin2021leveraging} show that including slot descriptions directly in prompts improves zero-shot dialogue state tracking by grounding the model in the semantics of each field. These studies highlight how prompts can embed domain knowledge to enhance extraction quality.

The choice between zero-shot and few-shot prompting also influences performance. Zero-shot prompts rely solely on schema descriptions and are flexible but less reliable. Few-shot prompting, which augments the prompt with exemplar input–output pairs, improves consistency but increases prompt length and inference latency. Pan et al.\ \cite{pan2023chatgpt} evaluate ChatGPT for dialogue understanding and find that few-shot prompting consistently improves accuracy, albeit with slower responses.

Schema-constrained prompting has also received attention as a way to enforce structured output. Zhang et al.\ \cite{zhang2023sgptod} introduced SGP-TOD, where prompts embed schema definitions for task-oriented dialogue systems. By instructing the model to generate values that must fit the schema, they reduce invalid or inconsistent outputs. Practical systems follow the same idea: Google’s LangExtract \cite{google2024langextract} uses user-defined JSON schemas to constrain extraction results. Although this improves format consistency, prompt-based schema enforcement remains monolithic, lacking modular correction and multi-model verification mechanisms.

Beyond manual prompt design, recent studies explore \emph{automated prompt optimization}. Peng et al.\ \cite{peng2023check} present a framework where candidate prompts are iteratively refined using feedback signals such as factual errors and schema mismatches. This “prompt tuning without training” approach suggests that prompts themselves can be optimized algorithmically, reducing reliance on human intuition.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/zero_shot_vs_few_shot.png}
    \caption{Illustration of zero-shot vs.\ few-shot prompting for template filling: zero-shot uses only task instructions, whereas few-shot adds example pairs that guide the model toward the correct structure, typically improving extraction accuracy.}
    \label{fig:zero-few-shot}
\end{figure}


\subsubsection{Evaluation Against Requirements}

The evaluation shows that prompt engineering provides a flexible and powerful way to adapt LLMs to structured extraction tasks. Zero-shot and few-shot prompting enable template filling without task-specific training, while structured and schema-guided prompts improve consistency and robustness. Automated prompt optimization further shows that prompt quality can be systematically refined.

Table~\ref{tab:eval-prompt-engineering} evaluates the prompt engineering approaches discussed above against requirements R1--R6. Each requirement uses the grading scale defined in Section~2.1.
\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{10pt}
\small
\begin{tabular}{|m{5cm}|c|c|c|c|c|c|}
\hline
\textbf{Solution} & \textbf{R1} & \textbf{R2} & \textbf{R3} & \textbf{R4} & \textbf{R5} & \textbf{R6} \\
\hline

In-context learning~\cite{brown2020language} &
\raisebox{-0.1cm}{\tikz[baseline]{\filldraw[fill=black] (0,0) -- (90:0.4cm)
arc (90:-150:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
--- \\
\hline

Structured prompting~\cite{sun2023slot} &
\raisebox{-0.1cm}{\tikz[baseline]{\filldraw[fill=black] (0,0) -- (90:0.4cm)
arc (90:-150:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\filldraw[fill=black]
(0,0) -- (90:0.4cm) arc (90:-150:0.4cm) -- cycle; \draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
--- \\
\hline

Schema-guided prompting~\cite{zhang2023sgprompt} &
\raisebox{-0.1cm}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
--- \\
\hline

Automated prompt optimization~\cite{peng2023prompt} &
\raisebox{-0.1cm}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\filldraw[fill=black] (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
\raisebox{-0.1cm}{\tikz[baseline]{\draw (0,0) circle (0.4cm);}} &
--- \\
\hline

\end{tabular}
\caption{Evaluation of prompt engineering approaches against requirements R1--R6.}
\label{tab:eval-prompt-engineering}
\end{table}


Despite these strengths, prompt engineering alone does not satisfy several core requirements identified in Section~2.1. None of the reviewed approaches support \textbf{transparency} (R3) through evidence tracing or confidence scores, nor do they provide \textbf{user correction} (R4) within the extraction loop. They also lack mechanisms for continuous \textbf{learning and adaptation} (R5), and none report \textbf{usability} measures (R6) such as latency or computational cost. Prompts additionally remain brittle: small phrasing changes can produce different outputs, and schema adherence often requires external validation.

Overall, while prompt engineering is an essential foundation, it remains insufficient for building reliable, transparent, and adaptive template-filling systems. These limitations motivate the next subsection on multi-agent LLM architectures, which introduce modular structure, delegation, and verification layers to overcome the constraints of single-prompt systems.
