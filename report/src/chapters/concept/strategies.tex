\section{Architectural Strategies}
\label{sec:architectural-strategies}

While the modular five-agent pipeline defines the general structure of the \textit{Invox} system, its effectiveness depends on how large language models (LLMs) are deployed within the \textit{Information Extraction} and \textit{Verification} stages. Different architectural strategies offer distinct trade-offs in terms of accuracy, robustness, latency, cost, and transparency. This section outlines four strategies and illustrates them using a common example to enable direct comparison.

For all examples, we use the following input derived from the MUC-4 benchmark:  
\textit{``On March 3, 1992, in Bogotá, a powerful car bomb exploded outside the Ministry of Defense, damaging nearby buildings and injuring 25 people, though no fatalities were reported. Authorities suspect a left-wing guerrilla group, but responsibility remains unconfirmed.''}

This example introduces multiple attributes (date, location, event type, casualties, suspected perpetrators, and epistemic uncertainty), allowing us to highlight the strengths and weaknesses of each strategy under realistic conditions.

\subsection{Strategy S1: Single-Pass (Full-Input, Single-LLM)}
\label{subsec:strategy-s1}

\textbf{Overview.}  
In the simplest approach, a single LLM receives the complete transcript—along with few-shot examples retrieved by the RAG agent—and directly generates all template fields in one inference pass. The extracted values then proceed through Consistency Formatting (CF) and Verification (VER) before finalization.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\linewidth]{images/single-llm-full-input.drawio.pdf}
  \caption{Strategy: Full-Input, Single-LLM}
  \label{fig:single-llm-full-input}
\end{figure}

\textbf{Advantages.}  
This strategy is computationally efficient, requiring only one IE model call. It minimizes latency and API costs, making it suitable for high-throughput scenarios. The inclusion of RAG-retrieved examples improves domain adaptation compared to zero-shot prompting, while CF and VER stages provide downstream quality assurance.

\textbf{Limitations.}  
If the LLM misinterprets a critical ambiguity—such as treating ``suspected guerrilla group'' as a confirmed perpetrator—the error propagates through CF (which normalizes the incorrect value) and may only be flagged by VER if confidence thresholds are exceeded. The single inference point offers no redundancy, making the system vulnerable to model-specific biases or hallucinations \cite{du2020event}.

\subsection{Strategy S2: Iterative (Slot-wise, Single-LLM)}
\label{subsec:strategy-s2}

\textbf{Overview.}  
Each template slot is extracted by an independent LLM call with a slot-specific prompt. Slots can be processed sequentially or in parallel. The RAG agent retrieves relevant examples once at the beginning, and these are included in every slot-level prompt. Results are then passed to CF and VER for normalization and validation.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\linewidth]{images/single-llm-slot-wise.drawio.pdf}
  \caption{Strategy: Slot-wise, Single-LLM}
  \label{fig:single-llm-slot-wise}
\end{figure}

\textbf{Advantages.}  
Slot independence isolates errors: if the perpetrator field is uncertain, only that slot requires re-extraction. This improves transparency (R3) by making it clear which fields are problematic. Parallelization across slots can reduce wall-clock latency on multi-core systems, though total computational cost increases. The strategy also supports slot-specific prompt engineering, allowing prompts to be tuned for date extraction, entity recognition, or casualty parsing independently.

\textbf{Limitations.}  
The approach increases inference cost linearly with the number of slots. For templates with 15–20 fields, this becomes expensive. Additionally, slot-level prompts lack cross-field context: the model extracting casualties does not see the perpetrator information, which may reduce coherence in cases where fields are interdependent \cite{sun2023slot}.

\subsection{Strategy S3: Multi-LLM Consensus (Full-Input)}
\label{subsec:strategy-s3}

\textbf{Overview.}  
Multiple LLMs—either different models (e.g., GPT-4, Claude, DeepSeek) or multiple runs of the same model with varied prompts—each generate a complete template from the full transcript and RAG-retrieved examples. Their outputs are compared by the Internal Verifier which has results from multiple llms, text input and the RAG examples, which selects the most consistent or reliable values using consensus rules such as majority voting or confidence-weighted aggregation.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\linewidth]{images/multiple-llm-full-input.drawio.pdf}
  \caption{Strategy: Full-Input, Multiple-LLM}
  \label{fig:multiple-llm-full-input}
\end{figure}

\textbf{Advantages.}  
Ensemble diversity reduces systematic bias: if one model hallucinates or misinterprets ambiguous phrasing, others may produce more faithful outputs. Consensus mechanisms increase robustness, particularly when models disagree on uncertain fields. This strategy also preserves cross-field context, as each model sees the full input during extraction \cite{wu2023autoagents, park2023generative}.

\textbf{Limitations.}  
Computational cost increases linearly with the number of models. If all models converge on the same misinterpretation—due to shared training biases or ambiguous input phrasing—consensus provides no advantage. The strategy also requires careful tuning of aggregation rules: simple majority voting may discard nuanced or hedged outputs (e.g., "suspected") in favor of overly confident but incorrect answers.

\subsection{Strategy S4: Multi-LLM Consensus (Slot-wise)}
\label{subsec:strategy-s4}

\textbf{Overview.}  
Each slot is extracted independently by multiple LLMs, and consensus is reached per field before aggregating results. This combines the granularity of S2 (slot-wise processing) with the robustness of S3 (multi-model consensus). Their outputs are compared by the Internal Verifier which has results from multiple llms, text input and the RAG examples, which selects the most consistent or reliable values using consensus rules such as majority voting or confidence-weighted aggregation. Results are then passed through CF and VER. 

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\linewidth]{images/multiple-llm-slot-wise.drawio.pdf}
  \caption{Strategy: Slot-wise, Multiple-LLM}
  \label{fig:multiple-llm-slot-wise}
\end{figure}

\textbf{Advantages.}  
This strategy offers the highest reliability by applying ensemble methods at the most granular level. Slot-level consensus allows diverse models to specialize: one model may excel at date parsing, another at casualty extraction. Error isolation remains strong, as each field is independently verified. The approach is well-suited for safety-critical domains where accuracy justifies cost.

\textbf{Limitations.}  
Computational cost scales as (number of slots) × (number of models), making this the most expensive strategy. For a 15-field template with 3 models, this requires 45 LLM calls per document. Latency increases unless aggressive parallelization is employed. Additionally, consensus logic becomes more complex when models produce semantically equivalent but syntactically different outputs (e.g., "25 injured" vs. "25 people hurt"), requiring robust normalization before voting.

\subsection{Summary and Trade-Off Analysis}
\label{subsec:strategy-summary}

These four strategies represent different points in the trade-off space between accuracy, robustness, latency, and cost. Table~\ref{tab:strategy-comparison} summarizes their characteristics.

\begin{table}[H]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Strategy} & \textbf{LLM Calls} & \textbf{Error Isolation} & \textbf{Robustness} & \textbf{Latency} & \textbf{Cost} \\
\midrule
S1: Single-Pass & 1 & L & L & L & L \\
S2: Iterative & $n$ (slots) & H & M & M & M \\
S3: Consensus (Full) & $m$ (models) & L & H & M & H \\
S4: Consensus (Slot) & $n \times m$ & H & H & H & H \\
\bottomrule
\end{tabular}
\caption{Comparison of architectural strategies. $n$ = number of slots, $m$ = number of models. L=Low, M=Medium, H=High.}
\label{tab:strategy-comparison}
\end{table}

\textbf{Selection criteria.} Strategy choice depends on deployment constraints:
\begin{itemize}
    \item \textbf{S1} is suitable for high-throughput applications where speed and cost matter more than perfect accuracy.
    \item \textbf{S2} enables fine-grained debugging and is appropriate when specific fields are known to be problematic.
    \item \textbf{S3} balances robustness and context preservation, making it suitable for general-purpose deployments.
    \item \textbf{S4} provides maximum reliability for critical applications where errors have serious consequences (e.g., medical records, regulatory reporting).
\end{itemize}

Chapter 5 empirically evaluates these strategies on the MUC-4 benchmark, also quantifying their performance across the six requirements (R1–R6) established in Chapter 2.\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{sa.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}
