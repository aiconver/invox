\section{System Architecture and Design}
\label{sec:concept-design}

This section presents the high-level architecture of the Invox system using the C4 model, which structures architectural documentation into hierarchical levels: context (system boundaries and external actors), containers (major components and their interactions), and process (dynamic execution flow). These views provide a comprehensive understanding of how the conceptual design translates into a deployable architecture.

\subsection{Context View: System Boundaries and External Actors}
\label{subsec:context-view}

\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/c4_context.drawio.pdf}
  \caption{Context diagram showing system boundaries and external actors.}
  \label{fig:c4-context}
\end{figure}

The context diagram (Figure~\ref{fig:c4-context}) positions Invox within its operational environment. The system interacts with three categories of external actors:

\textbf{Primary users} are domain experts—such as factory operators, medical staff, or administrative personnel—who provide unstructured input (speech or text) and review populated templates. They interact with the system through a web-based interface or mobile application.

\textbf{Data sources} include historical template repositories, domain-specific glossaries, and organizational knowledge bases. These are accessed by the RAG agent to retrieve contextually relevant examples. In regulated environments, these repositories may reside behind secure access controls.

\textbf{External services} consist of third-party APIs for LLM inference (OpenAI GPT-4, Anthropic Claude, DeepSeek), speech recognition (OpenAI Whisper), and search infrastructure (OpenSearch). The system communicates with these services over REST APIs.

\subsection{Container View: Internal Component Structure}
\label{subsec:container-view}

The container diagram (Figure~\ref{fig:c4-container}) decomposes Invox into its major internal components and clarifies how requests flow through the system.

\textbf{Client access.} Users interact with the system via the \emph{API Gateway}, which supports HTTP, HTTPS, or tRPC protocols. This is the only entry point for structured input requests. For raw audio, users may also call the \emph{STT Service} directly to obtain transcribed text.

\textbf{STT Service.} The STT service wraps the Whisper model, invokes the external ASR provider, and returns transcribed text enriched with metadata such as confidence scores, timestamps, and language detection. Transcribed text is returned to the client for display or routed back into the pipeline.

\textbf{RAG Service.} Once the API Gateway accepts a request, it forwards it into the backend pipeline starting with the RAG service. This service embeds the input, queries the \emph{Vector Index}, and enriches the request with semantically relevant examples.

\textbf{IE Service.} Using the enriched context, the IE service constructs prompts and invokes the configured LLM providers. Depending on the strategy (S1–S4), this may involve a single call, slot-wise calls, or multiple model invocations. Results are returned as candidate slot-value pairs.

\textbf{CF Service.} The CF service normalizes the extracted values (e.g., dates, entities) and enforces schema constraints to produce a consistent intermediate template.

\textbf{VER Service.} 
The verification service ensures completeness, consistency, and confidence of the template. It assigns confidence scores, detects conflicts, flags missing fields, and can trigger clarification requests via the API Gateway. Finalized templates, enriched with quality metadata, are stored in both the \emph{Template Repository} and the \emph{Vector Index}. This service is internal-only and not directly accessible to clients.

\textbf{Data stores.} 
\begin{itemize}
  \item \emph{Template Repository} stores finalized templates for future retrieval.
  \item \emph{Vector Index} holds dense embeddings for retrieval-augmented generation.
\end{itemize}

\textbf{End-to-end flow.} After verification, the finalized template is stored, indexed, and routed back through the API Gateway to the client as the official response.

\begin{figure}[H]
  \centering
  \includegraphics[width=1\linewidth]{images/c4_container.drawio.pdf}
  \caption{Corrected container diagram showing request flow through internal components and external dependencies.}
  \label{fig:c4-container}
\end{figure}

\subsection{Process View: Workflow and Decision Logic}
\label{subsec:process-view}

The process diagram (Figure~\ref{fig:bpmn}) presents the dynamic execution flow. The workflow follows this structure:

\textbf{Input reception.} User submits audio or text. Audio is processed by the STT service; text requests proceed via the API Gateway into the backend pipeline.

\textbf{Retrieval.} The RAG service queries the vector index and returns top-$k$ examples. If retrieval fails, the system falls back to zero-shot prompting.

\textbf{Extraction.} The IE service invokes the LLM(s) according to the chosen strategy. For multi-model strategies, all responses are collected before proceeding.

\textbf{Formatting.} CF normalizes extracted values and enforces schema constraints, flagging any that cannot be normalized for manual review.

\textbf{Verification.} VER checks consistency, completeness, and confidence. Possible outcomes are: (1) Pass—template is complete and returned; (2) Clarification required—system prompts user and re-executes IE $\rightarrow$ CF $\rightarrow$ VER; (3) Low confidence—template returned with uncertain fields for user review.

\textbf{Finalization.} Verified templates are stored in the repository, indexed for future retrieval, and returned to the client. Users may still edit fields before final submission.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\linewidth]{images/bpmn_process_flow.pdf}
  \caption{Process diagram showing workflow and decision logic.}
  \label{fig:bpmn}
\end{figure}
