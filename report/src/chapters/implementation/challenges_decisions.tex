\section{Challenges and Design Decisions}

The implementation of \textit{Invox} required several non-trivial design choices in order to balance robustness, cost, and development feasibility. This section highlights the most significant challenges encountered and the corresponding decisions.

\subsection*{Data Representation with JSON}
JSON was selected as the canonical communication format across agents due to its human readability, flexibility, and broad tool support. However, LLMs occasionally generate malformed or verbose JSON, which breaks downstream processing. To mitigate this, strict schema validation was applied at the backend, forcing models to conform to predefined templates. This decision improved stability but introduced additional validation overhead.

\subsection*{Handling Whisper Transcriptions}
The Whisper API was chosen for its robustness in noisy environments. Still, long audio segments occasionally exceeded context limits or produced partially truncated outputs. The backend therefore implements audio chunking with overlapping windows to preserve context across boundaries. While effective, this adds latency and increases storage requirements.

\subsection*{Choice of LLMs}
OpenAI ChatGPT and Google Gemini were integrated as interchangeable backends. This dual support allowed benchmarking and fallback in case of service degradation. The decision added engineering complexity, as prompts and output formats had to be harmonised across APIs. Nevertheless, the redundancy was considered worthwhile for reliability.

\subsection*{Verification Overheads}
Strategies involving verification (S3 and S4) required additional inference steps, which increased runtime and cost. The design decision was to make verification optional and configurable at runtime. This flexibility allows users to prioritise either efficiency or robustness depending on task criticality.

\subsection*{Authentication and Security}
Keycloak was selected for authentication and role-based access control, ensuring secure multi-user operation. The main challenge was integrating JSON-RPC communication with OAuth2 tokens. This was addressed by extending the tRPC layer with middleware that validates tokens before forwarding requests.

\subsection*{Development and Debugging Workflow}
Using a monorepo setup simplified dependency management across frontend, backend, and database code. VS Code and Postman were employed as primary tools for development and API testing. However, debugging multi-agent interactions proved complex due to asynchronous calls. To address this, structured logging and trace identifiers were introduced, enabling step-by-step replay of agent workflows.

\subsection*{Cost and Resource Constraints}
Running multiple LLM calls per template quickly accumulates costs. Design decisions therefore included:
\begin{itemize}
  \item caching results at the orchestrator layer,
  \item minimising redundant calls via early exits in verification,
  \item and batching slot queries where possible.
\end{itemize}
These measures significantly reduced API consumption without compromising correctness.

\subsection*{Summary}
The above challenges illustrate the trade-offs inherent in implementing modular, LLM-driven systems. Each decision reflects a compromise between competing objectives: reliability versus cost, transparency versus complexity, and robustness versus efficiency. These design experiences form the foundation for the evaluation in the next chapter.
